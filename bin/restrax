#!/usr/bin/env python
"""
Restrax: Rechunking live data
=============================================
How to use
----------------
    <activate conda environment>
    restrax --production

----------------

For more info, see the documentation:
https://straxen.readthedocs.io/en/latest/scripts.html
"""
__version__ = '0.1.0'

import argparse
import logging
import os
import immutabledict
import socket
import shutil
import time
import numpy as np
import strax
import straxen
import threading
import daqnt
import fnmatch
import typing as ty
from straxen import daq_core
from memory_profiler import memory_usage


def parse_args():
    parser = argparse.ArgumentParser(description="XENONnT rechunking manager")
    parser.add_argument('--production', action='store_true',
                        help="Run restrax in production mode, otherwise run in a test mode.")
    parser.add_argument('--ignore_checks', action='store_true',
                        help="Do not use! Skip checks before changing documents, there be dragons.", )
    parser.add_argument('--input_folder', type=str, default=daq_core.pre_folder,
                        help="Where to read the data from, should only be used when testing", )
    parser.add_argument('--max_threads', type=int, default=1,
                        help='max number of threads to simultaneously work on one run'
                        )
    parser.add_argument('--skip_compression', nargs='*', default=['*event*'],
                        help='skip recompression of any datatype that fnmatches. For example: "*event* *peak*"')
    # deep_compare is meant only for testing
    parser.add_argument('--deep_compare', action='store_true',
                        help='Open all the data of the old and the new format and check that they are the same')
    parser.add_argument('recompress_min_chunks', type=int, default = 3,
                        help ='Only bother with doing the recompression if there are more than this many chunks')
    actions = parser.add_mutually_exclusive_group()
    actions.add_argument('--undying', action='store_true',
                         help="Except any error and ignore it")
    actions.add_argument('--process', type=int,
                         help="Handle a single run")
    args = parser.parse_args()
    if args.input_folder != '/data/pre_processed/' and args.production:
        raise ValueError(
            "Thou shall not pass, don't upload files from non production folders, what are you thinking you're doing?!!")
    return args


def main():
    args = parse_args()
    while True:
        try:
            run_restrax(args)
            if args.process:
                break
        except (KeyboardInterrupt, SystemExit):
            break
        except Exception as fatal_error:
            # Open a class, just to log the error
            re_temp = ReStrax(args=args)
            re_temp.log.error(f'Fatal warning:\tran into {fatal_error}. Try '
                              f'logging error and restart restrax')
            try:
                re_temp.log_warning(
                    f'Fatal warning:\tran into {fatal_error}',
                    priority='error', )
            except Exception as warning_error:
                re_temp.error(f'Fatal warning:\tcould not log {warning_error}')

            if not args.undying:
                raise

            # This usually only takes a minute or two
            time.sleep(60)
            re_temp.warning('Restarting main loop')


def run_restrax(args):
    """Run the infinite loop of restrax"""
    restrax = ReStrax(args=args)
    restrax.infinite_loop(close=bool(args.process))


class ReStrax(daq_core.DataBases):
    """
    Restrax: rechunking the data from bootstrax and prepare it for admix/rucio
    """
    log: logging.Logger
    nap_time = 300  # s
    nap_time_short = 5  # s

    folders = immutabledict.immutabledict(
        production_out=daq_core.output_folder,
        test_out='/data/test_processed')

    raw_record_types = ('raw_records', 'raw_records_nv', 'raw_records_mv',
                        'raw_records_he',
                        'records', 'records_nv', 'records_mv')

    # If a dataset is larger than this many bites, only compare a single field (time)
    large_data_compare_threshold = 5e9

    # If the total data rate is lower than this, use bz2 for the compression of raw records.
    bz2_compression_threshold_mbs = 50  # MB/s

    def __init__(self, args):
        super().__init__(production=args.production)

        self.hostname = socket.getfqdn()
        self._set_logger()
        # Get from the database unless testing
        self.read_from = None if args.production else args.input_folder
        self.write_to = self.folders['production_out'] if args.production else self.folders['test_out']
        self.max_threads = args.max_threads
        self.ignore_checks = args.ignore_checks
        self.skip_compression = args.skip_compression
        self.process = args.process
        self.deep_compare = args.deep_compare
        self.recompress_min_chunks = args.recompress_min_chunks

    def infinite_loop(self, close=False) -> None:
        """Core of restrax, recompress the data followed by several validation steps"""
        while True:
            run_doc = self.find_work()
            if run_doc is None:
                if close:
                    return
                self.log.info('No work to do, sleep')
                self.take_a_nap()
                continue
            t0 = time.time()
            mem_for_doc = memory_usage(proc=(self.handle_run, (run_doc,), dict()))
            self.log.debug(f"Memory profiler says peak RAM usage was: {max(mem_for_doc):.1f} MB")
            self.log.debug(f'Took {(time.time() - t0) / 3600:.2f} h')
            self.log.info('Loop done')

    def find_work(self,
                  projection: ty.Optional[dict] = None
                  ) -> ty.Optional[dict]:
        """
        Get a list of documents to recompress and the associated rundoc

        :param projection: optional, which fields from the run document to query
        :return: list of data documents, and the run-document that
        """
        if projection is None:
            projection = {k: 1 for k in 'data number mode detectors'.split()}
        if self.production:
            return self._find_production_work(projection)
        return self._find_testing_work(projection)

    def _find_production_work(self, projection) -> ty.Optional[dict]:
        """Query the database for work"""
        query = {'status': 'eb_finished_pre',
                 'bootstrax.state': 'done',
                 'bootstrax.host': self.hostname
                 }
        if self.process:
            query['number'] = int(self.process)
        run_doc = self.run_coll.find_one(
            query,
            sort=[('_id', -1)],
            projection=projection
        )
        return run_doc

    def _find_testing_work(self, projection: dict) -> ty.Optional[dict]:
        """Find work from the pre-dir if we are testing"""
        folders = os.listdir(self.read_from)
        first_run = self.process
        data_docs = []
        for folder in folders:
            if os.path.exists(os.path.join(self.write_to, folder)):
                # Don't do work twice
                continue
            if len(split := folder.split('-')) and len(split[0]) == 6:
                run_id, data_type, lineage = split

                if first_run is None:
                    first_run = run_id
                if run_id != first_run:
                    continue
                self.log.info(f'Do {folder}')
                data_docs.append({'host': self.hostname,
                                  'location': os.path.join(self.read_from, folder),
                                  'type': data_type,
                                  'linage_hash': lineage,
                                  })
        if not len(data_docs):
            return None
        run_doc = self.run_col.find_one({'number': int(first_run)}, projection=projection)
        run_doc['data'] = data_docs
        return run_doc

    def handle_run(self, run_doc):
        """For a given batch of data_docs of a given run, do all the rechunking steps"""
        
        data_docs = self._get_data_docs(run_doc)
        run_doc['data'] = data_docs
        self.log.info(f'{run_doc["number"]} -> doing {len(data_docs)}')
        # self.run_software_veto(run_doc)
        self.rechunk_docs(data_docs)
        self.do_checks(data_docs)
        if self.deep_compare:
            self.do_work(data_docs, function=self._validate_data_per_doc)
        self.finalize_execute(data_docs)
    
    def _get_data_docs(self, run_doc):
        """Extract data doc from rundoc and sort by largest first"""
        # Filter data documents that are only on this host
        data_docs = [data_doc for data_doc in run_doc['data'] if
                     data_doc.get('host') == self.hostname
                     ]
        storage_backend = strax.FileSytemBackend()
        size = lambda data_doc: sum(
            chunk.get('nbytes', 0) for chunk in 
            storage_backend.get_metadata(data_doc['location']).get('chunks', [dict()])
        )
        data_docs = sorted(data_docs, key=size, reverse=True)
        return data_docs
        
    def run_software_veto(self, run_doc):
        """This is where we can add a software veto for specific runs"""
        raise NotImplementedError

    def rechunk_docs(self, docs: ty.List[dict]) -> None:
        """
        For each of the data documents, rechunk/recompress/move/check the data,
        if multi-threading is allowed, use that
        :param docs: list of data documents to rechunk
        """
        if self.max_threads > 1:
            threads = []
            for next_doc in docs:
                self._sleep_while_n_threads_alive(threads, self.max_threads - 1)
                thread = threading.Thread(
                    target=self._rechunk_per_doc,
                    args=(next_doc,))
                thread.start()
                threads.append(thread)
            # Wait until all threads have finished
            self._sleep_while_n_threads_alive(threads, 0)
        else:
            for ddoc in docs:
                self._rechunk_per_doc(ddoc)

    def _rechunk_per_doc(self, run_doc, data_doc):
        """Do the rechunking document by document"""
        dir_in = data_doc['location']
        dir_out = self.renamed_path(dir_in)

        if not os.path.exists(dir_in):
            raise FileNotFoundError(data_doc)

        if self.should_skip_compression(data_doc):
            # Just copy, don't read convert write
            # We could do shutil.move here, but... what if we fail while moving?
            # Also, a bit harder with doing the datadocs right
            self.log.info(f'Copy (skip recompression) for {dir_in}')
            if os.path.exists(dir_out):
                self._remove_dir(dir_out)
            shutil.copytree(dir_in, dir_out, dirs_exist_ok=True)
            return

        self.log.info(f'Start {dir_in} -> {dir_out}')
        compressor, target_size_mb = self.get_compressor_and_size(run_doc, data_doc)

        summary = strax.rechunker(
            source_directory=dir_in,
            dest_directory=self.write_to,
            replace=False,
            compressor=compressor,
            target_size_mb=target_size_mb,
            rechunk=True,
        )
        self.log.info(f'{dir_out} written {summary}')

    def _sleep_while_n_threads_alive(self, threads, n):
        while sum(t.is_alive() for t in threads) > n:
            self.take_a_nap(self.nap_time_short)

    def get_compressor_and_size(self, run_doc: dict, data_doc: dict) -> ty.Tuple[str, ty.Optional[int]]:
        """
        For a given data document infer the desired compressor, and target size

        :param run_doc: run document
        :param data_doc: data document
        """
        # This is where we might do some fancy coding
        dtype = data_doc['type']
        if dtype in self.raw_record_types:
            rate = sum(detector.get('avg', 100) for detector in run_doc.get('rate', {'none': {}}).values())
            compressor = 'bz2' if rate < self.bz2_compression_threshold_mbs else 'zstd'
            target_size_mb = 5000
        elif 'peak' in dtype:
            compressor = 'zstd'
            target_size_mb = 1500
        else:
            # Use default compressor
            compressor = None
            target_size_mb = 1500
        self.log.debug(f'Setting {dtype}: {compressor}, {target_size_mb} MB')
        return compressor, target_size_mb

    def should_skip_compression(self, data_doc) -> bool:
        """Should we skip recompressing this data? For example if the data is
        already so small that it's not worth recompressing

        :param data_doc: data document
        """
        if data_doc['type'] == 'live_data':
            return True
        if any(fnmatch.fnmatch(data_doc['type'], delete)
               for delete in self.skip_compression):
            return True
        n_chunks = len(strax.FileSytemBackend().get_metadata(data_doc['location']).get('chunks', []))
        if n_chunks <= self.recompress_min_chunks:
            # no need to recompress data if it's only one chunk
            return True
        return False

    def do_checks(self, data_docs: ty.List[dict]) -> None:
        """
        Open the metadata of the old and the new file to check if everything
        is consistent and no exceptions were encountered during recompression

        :param data_docs: list of data documents to rechunk
        """
        if self.ignore_checks:
            # One does not just venture into Mar a lago!
            return
        storage_backend = strax.FileSytemBackend()
        errors = []
        for data_doc in data_docs:
            dir_in = data_doc['location']
            dir_out = self.renamed_path(dir_in)

            if not os.path.exists(dir_in): errors.append(f'{dir_in} does not exists.')
            if not os.path.exists(dir_out): errors.append(f'{dir_out} does not exists.')
            md_in = storage_backend.get_metadata(dir_in)
            md_out = storage_backend.get_metadata(dir_out)

            if 'exception' in md_out:
                errors.append('Writing error!')

            if sum(chunk['n'] for chunk in md_in['chunks']) != sum(chunk['n'] for chunk in md_out['chunks']):
                errors.append('Rechunked data has fewer entries?!')
        if errors:
            raise ValueError(f'Doc {data_docs} had errors: ' + ' and '.join(errors))
        self.log.info('All checks passed')

    def _validate_data_per_doc(self, data_doc):
        """Open the data from the old and new source, compare to assert it's the same"""
        storage_backend = strax.FileSytemBackend()
        dir_in = data_doc['location']
        dir_out = self.renamed_path(dir_in)

        md_in = storage_backend.get_metadata(dir_in)
        is_large_dset = sum(chunk['nbytes'] for chunk in md_in['chunks']) > self.large_data_compare_threshold

        self.log.info(f'Checking {dir_in} vs {dir_out}. Is large {is_large_dset}')
        kw = dict(keep_column='time' if is_large_dset else None, progress_bar=is_large_dset)
        data_in = self._get_data_from_dir(dir_in, **kw)
        data_out = self._get_data_from_dir(dir_out, **kw)
        errors = self._compare_data(data_in, data_out)
        if errors:
            raise ValueError(f'Data was not the same when comparing {dir_in} {dir_out}. See:\n' + '\n'.join(errors))
        self.log.debug('Compare done')

    @staticmethod
    def _compare_data(data_in, data_out):
        """Compare structured/simple numpy arrays"""
        errors = []
        if data_in.dtype.names:
            for field in data_in.dtype.names:
                if 'str' in data_in.dtype[field].name:
                    # NB! The equal_nan compare does not work for string fields!
                    if not np.array_equal(data_in[field], data_out[field]):
                        errors.append(f'Error for "{field}"')
                elif not np.array_equal(data_in[field], data_out[field], equal_nan=True):
                    errors.append(f'Error for "{field}"')
        elif not np.array_equal(data_in, data_out, equal_nan=True):
            errors.append('Data not the same')
        return errors

    def _get_data_from_dir(self, folder, keep_column=None, progress_bar=False) -> np.ndarray:
        """
        Load data from a specified folder
        :param folder: absolute path to folder
        :param keep_column: columns to keep from structured array (if None, get all)
        :param progress_bar: show a tqdm progressbar
        :return: concatenated numpy array
        """
        self.log.debug(f'Load {folder}, keep_column = "{keep_column if keep_column else "ALL"}"')
        loader = strax.utils.tqdm(strax.FileSytemBackend().loader(folder), disable=not progress_bar,
                                  desc=folder.split('/')[-1])
        if keep_column:
            return np.concatenate([chunk.data[keep_column] for chunk in loader])
        return np.concatenate([chunk.data for chunk in loader])

    def finalize_execute(self, data_docs: ty.List[dict]) -> None:
        """
        Update the database according to the operation and update the metadata

        :param data_docs: list of data documents that have been rechunk
        :return:
        """
        # Maybe could merge this with do checks? -> Avoid opening metadate twice?
        # Then again, that is SO minor in the grand scheme of things, that I just leave it like this for the moment
        if not self.production or not len(data_docs):
            return
        storage_backend = strax.FileSytemBackend()
        for data_doc in data_docs:
            if self.hostname != data_doc['host']:
                continue
            dir_in = data_doc.get('location', '')
            dir_out = self.renamed_path(dir_in)

            new_metadata = storage_backend.get_metadata(dir_out)
            run_id = new_metadata['run_id']
            chunk_mb = [chunk['nbytes'] / 1e6 for chunk in new_metadata['chunks']]

            if not self.production:
                continue

            self.log.info(f'Update data doc {dir_out}')
            self.run_coll.update_one(
                {'number': int(run_id),
                 'data': {
                     '$elemMatch': {
                         'location': data_doc['location'],
                         'host': data_doc['host']}}},
                {'$set':
                     {'data.$.location': dir_out,
                      'data.$.file_count': len(os.listdir(dir_out)),
                      'data.$.meta.strax_version': strax.__version__,
                      'data.$.meta.straxen_version': straxen.__version__,
                      'data.$.meta.size_mb': int(np.sum(chunk_mb)),
                      'data.$.meta.avg_chunk_mb': int(np.average(chunk_mb)),
                      'data.$.meta.lineage_hash': new_metadata['lineage_hash'],
                      'data.$.meta.compressor': new_metadata['compressor'],
                      }
                 })
        # Mark as ready for upload such that admix can take over from here
        if self.production:
            self.run_coll.update_one(
                {'number': int(run_id)},
                {'$set': {'status': 'eb_ready_to_upload'}})
        self.log.info('Rundoc updated')

    def take_a_nap(self, dt=None):
        time.sleep(dt if dt is not None else self.nap_time)

    def renamed_path(self, path):
        return os.path.join(self.write_to, os.path.split(path)[-1])

    def _set_logger(self) -> None:
        versions = straxen.print_versions(
            modules='strax straxen utilix daqnt numpy tensorflow numba'.split(),
            include_git=True,
            return_string=True,
        )
        log_name = 'restrax' + self.hostname + ('' if self.production else '_TESTING')
        self.log = daqnt.get_daq_logger(
            log_name,
            log_name,
            level=logging.DEBUG,
            opening_message=f'Welcome to restrax\n{versions}')

    def _remove_dir(self, directory):
        """Remove directory (when in production mode)"""
        self.log.info(f'Remove {directory}')
        if self.production:
            shutil.rmtree(directory)

    def log_warning(self, message, **kw):
        self.log.warning(message)
        for key, value in dict(production=self.production, user=f'restrax_{self.hostname}').items():
            kw.setdefault(key, value)
        super().log_warning(message, **kw)


if __name__ == '__main__':
    start = time.time()
    mem = memory_usage(proc=(main,))
    print(f"Memory profiler says peak RAM usage was: {max(mem):.1f} MB")
    print(f'Took {time.time() - start:.1f} s = {(time.time() - start) / 3600:.2f} h ')
    print('Bye, bye')
