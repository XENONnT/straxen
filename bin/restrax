#!/usr/bin/env python
"""
Restrax: Rechunking live data
=============================================
How to use
----------------
    <activate conda environment>
    restrax --production

----------------

For more info, see the documentation:
https://straxen.readthedocs.io/en/latest/scripts.html
"""
__version__ = '0.0.2'

import argparse
import logging
import os
import immutabledict
import socket
import shutil
import time
import numpy as np
import strax
import straxen
import threading
import daqnt
import fnmatch
import typing as ty
from straxen import daq_core
from memory_profiler import memory_usage


def parse_args():
    parser = argparse.ArgumentParser(description="XENONnT rechunking manager")
    parser.add_argument('--production', action='store_true',
                        help="Run restrax in production mode, otherwise run in a test mode.")
    parser.add_argument('--ignore_checks', action='store_true',
                        help="Do not use! Skip checks before changing documents, there be dragons.", )
    parser.add_argument('--input_folder', type=str, default=daq_core.pre_folder,
                        help="Where to read the data from, should only be used when testing", )
    parser.add_argument('--max_threads', type=int, default=1,
                        help='max number of threads to simultaneously work on one run'
                        )
    parser.add_argument('--skip_compression', nargs='*',
                        default=['*event*', '*peak*'],
                        help='skip recompression of any datatype that fnmatches'
                        )
    parser.add_argument('--deep_compare', action='store_true',
                        help='open all the data of the old and the new format and check that they are the same'
                        )
    actions = parser.add_mutually_exclusive_group()
    actions.add_argument('--undying', action='store_true',
                         help="Except any error and ignore it")
    actions.add_argument('--process', type=int,
                         help="Handle a single run")
    args = parser.parse_args()
    if args.input_folder != '/data/pre_processed/' and args.production:
        raise ValueError(
            "Thou shall not pass, don't upload files from non production folders, what are you thinking you're doing?!!")
    return args


def main():
    args = parse_args()
    while True:
        try:
            run_restrax(args)
            if args.process:
                break
        except (KeyboardInterrupt, SystemExit):
            break
        except Exception as fatal_error:
            # Open a class, just to log the error
            re_temp = ReStrax(args=args)
            re_temp.log.error(f'Fatal warning:\tran into {fatal_error}. Try '
                              f'logging error and restart restrax')
            try:
                re_temp.log_warning(
                    f'Fatal warning:\tran into {fatal_error}',
                    priority='error', )
            except Exception as warning_error:
                re_temp.error(f'Fatal warning:\tcould not log {warning_error}')

            if not args.undying:
                raise

            # This usually only takes a minute or two
            time.sleep(60)
            re_temp.warning('Restarting main loop')


def run_restrax(args):
    """Run the infinite loop of restrax"""
    restrax = ReStrax(args=args)
    restrax.infinite_loop(close=bool(args.process))


class ReStrax(daq_core.DataBases):
    """
    Restrax: rechunking the data from bootstrax and prepare it for admix/rucio
    """
    log: logging.Logger
    nap_time = 300  # s
    nap_time_short = 5  # s

    folders = immutabledict.immutabledict(
        production_out=daq_core.output_folder,
        test_out='/data/test_processed')

    raw_record_types = ('raw_records', 'raw_records_nv', 'raw_records_mv',
                        'raw_records_he',
                        'records', 'records_nv', 'records_mv')

    # If a dataset is larger than this many bites, only compare a single field
    large_data_compare_threshold = 5e9

    def __init__(self, args):
        super().__init__(production=args.production)

        self.hostname = socket.getfqdn()
        self._set_logger()
        # Get from the database unless testing
        self.read_from = None if args.production else args.input_folder
        self.write_to = self.folders['production_out'] if args.production else self.folders['test_out']
        self.max_threads = args.max_threads
        self.ignore_checks = args.ignore_checks
        self.skip_compression = args.skip_compression
        self.process = args.process

    def infinite_loop(self, close=False) -> None:
        """Core of restrax, recompress the data followed by several validation steps"""
        while True:
            ddocs = self.find_work()
            if not ddocs:
                if close:
                    return
                self.log.info('No work to do, sleep')
                self.take_a_nap()
                continue
            self.rechunk_docs(ddocs)
            self.do_checks(ddocs)
            if self.deep_compare:
                self.do_work(ddocs, function=self._validate_data_per_doc)
            self.finalize_execute(ddocs)
            self.log.info('Loop done')

    def find_work(self) -> ty.Optional[ty.List[dict]]:
        """Get a list of documents to recompress"""
        if self.production:
            query = {'status': 'eb_finished_pre',
                     'bootstrax.state': 'done',
                     'bootstrax.host': self.hostname
                     }
            if self.process:
                query['number'] = int(self.process)
            rund_doc = self.run_coll.find_one(
                query,
                sort=[('_id', -1)],
                projection={'data': 1, 'number': 1}
            )
            if rund_doc is None:
                return None
            ddocs = [doc for doc in rund_doc['data'] if
                     doc.get('host') == self.hostname
                     ]
            self.log.info(f'{rund_doc["number"]} -> doing {len(ddocs)}')
            return ddocs

        # Just try to do all the folders, we are doing a test
        folders = os.listdir(self.read_from)
        first_run = None
        ddocs = []
        for folder in folders:
            if os.path.exists(os.path.join(self.write_to, folder)):
                continue
            if len(split := folder.split('-')) and len(split[0]) == 6:
                run_id, data_type, lineage = split
                if first_run is None:
                    first_run = run_id
                if run_id != first_run:
                    continue
                self.log.info(f'Do {folder}')
                ddocs.append({'host': self.hostname,
                              'location': os.path.join(self.read_from, folder),
                              'type': data_type,
                              'linage_hash': lineage,
                              })
        self.log.info(f'Total of {len(ddocs)} data documents')
        return ddocs

    def rechunk_docs(self, docs: ty.List[dict]) -> None:
        """
        For each of the data documents, rechunk/recompress/move/check the data,
        if multi-threading is allowed, use that
        :param docs: list of data documents to rechunk
        """
        if self.max_threads > 1:
            threads = []
            for next_doc in docs:
                self._sleep_while_n_threads_alive(threads, self.max_threads - 1)
                thread = threading.Thread(
                    target=self._rechunk_per_doc,
                    args=(next_doc,))
                thread.start()
                threads.append(thread)
            # Wait until all threads have finished
            self._sleep_while_n_threads_alive(threads, 0)
        else:
            for ddoc in docs:
                self._rechunk_per_doc(ddoc)

    def _sleep_while_n_threads_alive(self, threads, n):
        while (n_alive := sum(t.is_alive() for t in threads)) > n:
            if n == 0:
                self.log.debug(f'{n_alive} threads alive, sleep')
            self.take_a_nap(self.nap_time_short)

    def _rechunk_per_doc(self, ddoc):
        """Do the rechunking document by document"""
        dir_in = ddoc['location']
        dir_out = self.renamed_path(dir_in)

        if not os.path.exists(dir_in):
            raise FileNotFoundError(ddoc)

        if self.should_skip_compression(ddoc):
            # Just copy, don't read convert write
            # We could do shutil.move here, but... what if we fail while moving?
            # Also, a bit harder with doing the datadocs right
            self.log.info(f'Copy (skip recompression) for {dir_in}')
            if os.path.exists(dir_out):
                self._remove_dir(dir_out)
            shutil.copytree(dir_in, dir_out, dirs_exist_ok=True)
            return

        self.log.info(f'Start {dir_in} -> {dir_out}')
        compressor, target_size_mb = self.get_compressor_and_size(ddoc)

        summary = strax.rechunker(
            source_directory=dir_in,
            dest_directory=self.write_to,
            replace=False,
            compressor=compressor,
            target_size_mb=target_size_mb,
            rechunk=True,
        )
        self.log.info(f'{dir_out} written {summary}')

    def get_compressor_and_size(self, ddoc: dict) -> ty.Tuple[str, ty.Optional[int]]:
        """
        For a given data document infer the desired compressor, and target size

        :param ddoc: data document
        """
        # This is where we might do some fancy coding
        dtype = ddoc['type']
        if dtype in self.raw_record_types:
            compressor = 'bz2'
            target_size_mb = 5000
        elif dtype in ddoc['type']:
            compressor = 'zstd'
            target_size_mb = 1000
        else:
            # Use default compressor
            compressor = None
            target_size_mb = 1000
        self.log.debug(f'Setting {dtype}: {compressor}, {target_size_mb} MB')
        return compressor, target_size_mb

    def should_skip_compression(self, ddoc) -> bool:
        """Should we skip recompressing this data? For example if the data is
        already so small that it's not worth recompressing

        :param ddoc: data document
        """
        if ddoc['type'] == 'live_data':
            return True
        return any(fnmatch.fnmatch(ddoc['type'], delete)
                   for delete in self.skip_compression)

    def do_checks(self, docs: ty.List[dict]) -> None:
        """
        Open the metadata of the old and the new file to check if everything
        is consistent and no exceptions were encountered during recompression

        :param docs: list of data documents to rechunk
        """
        if self.ignore_checks:
            # One does not just venture into Mar a lago!
            return
        storage_backend = strax.FileSytemBackend()
        errors = []
        for ddoc in docs:
            dir_in = ddoc['location']
            dir_out = self.renamed_path(dir_in)

            if not os.path.exists(dir_in): errors.append(f'{dir_in} does not exists.')
            if not os.path.exists(dir_out): errors.append(f'{dir_out} does not exists.')
            md_in = storage_backend.get_metadata(dir_in)
            md_out = storage_backend.get_metadata(dir_out)

            if 'exception' in md_out:
                errors.append('Writing error!')

            if sum(chunk['n'] for chunk in md_in['chunks']) != sum(chunk['n'] for chunk in md_out['chunks']):
                errors.append('Rechunked data has fewer entries?!')
        if errors:
            raise ValueError(f'Doc {ddoc} had errors: ' + ' and '.join(errors))
        self.log.info('All checks passed')

    def _validate_data_per_doc(self, ddoc):
        """Open the data from the old and new source, compare to assert it's the same"""
        storage_backend = strax.FileSytemBackend()
        dir_in = ddoc['location']
        dir_out = self.renamed_path(dir_in)

        md_in = storage_backend.get_metadata(dir_in)
        is_large_dset = sum(chunk['nbytes'] for chunk in md_in['chunks']) > self.large_data_compare_threshold

        self.log.info(f'Checking {dir_in} vs {dir_out}. Is large {is_large_dset}')
        kw = dict(keep_column='time' if is_large_dset else None, progress_bar=is_large_dset)
        data_in = self._get_data_from_dir(dir_in, **kw)
        data_out = self._get_data_from_dir(dir_out, **kw)
        errors = self._compare_data(data_in, data_out)
        if errors:
            raise ValueError(f'Data was not the same when comparing {dir_in} {dir_out}. See:\n' + '\n'.join(errors))
        self.log.debug('Compare done')

    @staticmethod
    def _compare_data(data_in, data_out):
        """Compare structured/simple numpy arrays"""
        errors = []
        if data_in.dtype.names:
            for field in data_in.dtype.names:
                if 'str' in data_in.dtype[field].name:
                    # NB! The equal_nan compare does not work for string fields!
                    if not np.array_equal(data_in[field], data_out[field]):
                        errors.append(f'Error for "{field}"')
                elif not np.array_equal(data_in[field], data_out[field], equal_nan=True):
                    errors.append(f'Error for "{field}"')
        elif not np.array_equal(data_in, data_out, equal_nan=True):
            errors.append('Data not the same')
        return errors

    def _get_data_from_dir(self, folder, keep_column=None, progress_bar=False) -> np.ndarray:
        """
        Load data from a specified folder
        :param folder: absolute path to folder
        :param keep_column: columns to keep from structured array (if None, get all)
        :param progress_bar: show a tqdm progressbar
        :return: concatenated numpy array
        """
        self.log.debug(f'Load {folder}, keep_column = "{keep_column if keep_column else "ALL"}"')
        loader = strax.utils.tqdm(strax.FileSytemBackend().loader(folder), disable=not progress_bar,
                                  desc=folder.split('/')[-1])
        if keep_column:
            return np.concatenate([chunk.data[keep_column] for chunk in loader])
        return np.concatenate([chunk.data for chunk in loader])

    def finalize_execute(self, docs: ty.List[dict]) -> None:
        """
        Update the database according to the operation and update the metadata

        :param docs: list of data documents that have been rechunk
        :return:
        """
        # Maybe could merge this with do checks? -> Avoid opening metadate twice?
        # Then again, that is SO minor in the grand scheme of things, that I just leave it like this for the moment
        if not self.production or not len(docs):
            return
        storage_backend = strax.FileSytemBackend()
        for ddoc in docs:
            if self.hostname != ddoc['host']:
                continue
            dir_in = ddoc.get('location', '')
            dir_out = self.renamed_path(dir_in)
            file_count = len(os.listdir(dir_out))
            md = storage_backend.get_metadata(dir_out)
            run_id = md['run_id']
            chunk_mb = [chunk['nbytes'] / 1e6 for chunk in md['chunks']]
            data_size_mb = int(np.sum(chunk_mb))
            avg_data_size_mb = int(np.average(chunk_mb))
            lineage_hash = md['lineage_hash']
            compressor = md['compressor']

            if not self.production:
                continue

            self.log.info(f'Update ddoc {dir_out}')
            self.run_coll.update_one(
                {'number': int(run_id),
                 'data': {
                     '$elemMatch': {
                         'location': ddoc['location'],
                         'host': ddoc['host']}}},
                {'$set':
                 # TODO check that this new location is correct!
                     {'data.$.location': dir_out,
                      'data.$.file_count': file_count,
                      'data.$.meta.strax_version': strax.__version__,
                      'data.$.meta.straxen_version': straxen.__version__,
                      'data.$.meta.size_mb': data_size_mb,
                      'data.$.meta.avg_chunk_mb': avg_data_size_mb,
                      'data.$.meta.lineage_hash': lineage_hash,
                      'data.$.meta.compressor': compressor,
                      }
                 })
        # Mark as ready for upload such that admix can take over from here
        if self.production:
            self.run_coll.update_one(
                {'number': int(run_id)},
                {'$set': {'status': 'eb_ready_to_upload'}})
        self.log.info('Rundoc updated')

    def take_a_nap(self, dt=None):
        time.sleep(dt if dt is not None else self.nap_time)

    def renamed_path(self, path):
        return os.path.join(self.write_to, os.path.split(path)[-1])

    def _set_logger(self) -> None:
        versions = straxen.print_versions(
            modules='strax straxen utilix daqnt numpy tensorflow numba'.split(),
            include_git=True,
            return_string=True,
        )
        log_name = 'restrax' + self.hostname + ('' if self.production else '_TESTING')
        self.log = daqnt.get_daq_logger(
            log_name,
            log_name,
            level=logging.DEBUG,
            opening_message=f'Welcome to restrax\n{versions}')

    def _remove_dir(self, directory):
        """Remove directory (when in production mode)"""
        self.log.info(f'Remove {directory}')
        if self.production:
            shutil.rmtree(directory)

    def log_warning(self, message, **kw):
        self.log.warning(message)
        for key, value in dict(production=self.production, user=f'restrax_{self.hostname}').items():
            kw.setdefault(key, value)
        super().log_warning(message, **kw)


if __name__ == '__main__':
    start = time.time()
    mem = memory_usage(proc=(main,))
    print(f"Memory profiler says peak RAM usage was: {max(mem):.1f} MB")
    print(f'Took {time.time() - start:.1f} s = {(time.time() - start) / 3600:.2f} h ')
    print('Bye, bye')
