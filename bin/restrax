#!/usr/bin/env python
"""
Restrax: Rechunking live data
=============================================
How to use
----------------
    <activate conda environment>
    restrax --production

----------------

For more info, see the documentation:
https://straxen.readthedocs.io/en/latest/scripts.html
"""
__version__ = '0.0.1'

import argparse
import logging
import os
import immutabledict
import socket
import shutil
import time
import numpy as np
import strax
import straxen
import threading
import daqnt
import fnmatch
from straxen import daq_core
from memory_profiler import memory_usage


def parse_args():
    parser = argparse.ArgumentParser(description="XENONnT rechunking manager")
    parser.add_argument('--production', action='store_true',
                        help="Run restrax in production mode.")
    parser.add_argument('--ignore_checks', action='store_true',
                        help="Do not use! Skip checks before changing documents there be dragons.")
    parser.add_argument('--input_folder', type=str, default=daq_core.pre_folder,
                        help="Where to read the data from, should only be used when testing", )
    parser.add_argument('--max_threads', type=int, default=1,
                        help='max number of threads to simultaneously work on one run'
                        )
    parser.add_argument('--skip_compression', nargs='*',
                        default=['*event*', '*peak*'],
                        help='skip recompression of any datatype that fnmatches'
                        )
    actions = parser.add_mutually_exclusive_group()
    actions.add_argument('--undying', action='store_true',
                         help="Except any error and ignore it")
    actions.add_argument('--process', type=int,
                         help="Handle a single run")
    args = parser.parse_args()
    if args.input_folder != '/data/pre_processed/' and args.production:
        raise ValueError(
            "Thou shall not pass, don't upload files from non production folders, what are you thinking you're doing?!!")
    return args


def main():
    args = parse_args()
    while True:
        try:
            run_restrax(args)
            if args.process:
                break
        except (KeyboardInterrupt, SystemExit):
            break
        except Exception as fatal_error:
            # Open a class, just to log the error
            re_temp = ReStrax(args=args)
            re_temp.log.error(f'Fatal warning:\tran into {fatal_error}. Try '
                              f'logging error and restart restrax')
            try:
                re_temp.log_warning(
                    f'Fatal warning:\tran into {fatal_error}',
                    priority='error', )
            except Exception as warning_error:
                re_temp.error(f'Fatal warning:\tcould not log {warning_error}')

            if not args.undying:
                raise

            # This usually only takes a minute or two
            time.sleep(60)
            re_temp.warning('Restarting main loop')


def run_restrax(args):
    """Run the infinite loop of restrax"""
    restrax = ReStrax(args=args)
    restrax.infinite_loop(close=bool(args.process))


class ReStrax(daq_core.DataBases):
    """
    Restrax: rechunking the data from bootstrax and prepare it for admix/rucio
    """
    log: logging.Logger
    nap_time = 300  # s
    nap_time_short = 5  # s
    folders = immutabledict.immutabledict(
        production_out=daq_core.output_folder,
        test_out='/data/test_processed')

    raw_record_types = ('raw_records', 'raw_records_nv', 'raw_records_mv',
                        'records', 'records_nv', 'records_mv')
    deep_compare = True

    def __init__(self, args):
        super().__init__(production=args.production)

        self.hostname = socket.getfqdn()
        self.set_logger()
        if args.production:
            self.read_from = None  # get from the database
            self.write_to = self.folders['production_out']
        else:
            self.read_from = args.input_folder
            self.write_to = self.folders['test_out']
        self.max_threads = args.max_threads
        self.trump_mode = args.ignore_checks
        self.skip_compression = args.skip_compression
        self.process = args.process

    def infinite_loop(self, close=False):
        while True:
            ddocs = self.find_work()
            if not ddocs:
                if close:
                    return
                self.log.info('No work to do, sleep')
                self.take_a_nap()
                continue
            self.do_work(ddocs)
            self.do_checks(ddocs)
            self.finalize_execute(ddocs)
            self.log.info('Loop done')

    def find_work(self):
        if self.production:
            query = {'status': 'eb_finished_pre',
                     'bootstrax.state': 'done',
                     'bootstrax.host': self.hostname
                     }
            if self.process:
                query['number'] = int(self.process)
            rund_doc = self.run_coll.find_one(
                query,
                sort=[('_id', -1)],
                projection={'data': 1, 'number': 1}
            )
            if rund_doc is None:
                return None
            ddocs = [doc for doc in rund_doc['data'] if
                     doc.get('host') == self.hostname
                     ]
            self.log.info(f'{rund_doc["number"]} -> doing {len(ddocs)}')
            return ddocs

        # Just try to do all the folders, we are just doing a test
        folders = os.listdir(self.read_from)
        first_run = None
        ddocs = []
        for folder in folders:
            if os.path.exists(os.path.join(self.write_to, folder)):
                continue
            if len(split := folder.split('-')) and len(split[0]) == 6:
                run_id, data_type, lineage = split
                if first_run is None:
                    first_run = run_id
                if run_id != first_run:
                    continue
                self.log.info(f'Do {folder}')
                ddocs.append({'host': self.hostname,
                              'location': os.path.join(self.read_from, folder),
                              'type': data_type,
                              'linage_hash': lineage,
                              })
        self.log.info(f'Total of {len(ddocs)} data documents')
        return ddocs

    def do_work(self, ddocs):
        """ For each of the data documents, rechunk/recompress/move the data """
        if self.max_threads > 1:
            threads = []
            for next_doc in ddocs:
                self._sleep_while_n_threads_alive(threads, self.max_threads)
                thread = threading.Thread(
                    target=self._do_work_per_ddoc,
                    args=(self, next_doc))
                thread.start()
                threads.append(thread)
            # Wait until all threads have finished
            self._sleep_while_n_threads_alive(threads, 0)
        else:
            for ddoc in ddocs:
                self._do_work_per_ddoc(ddoc)

    def _sleep_while_n_threads_alive(self, threads, n):
        while sum(t.is_alive() for t in threads) >= n:
            self.take_a_nap(self.nap_time_short)

    def _do_work_per_ddoc(self, ddoc):
        dir_in = ddoc['location']
        dir_out = self.renamed_path(dir_in)

        if not os.path.exists(dir_in):
            raise FileNotFoundError(ddoc)

        if self.should_skip_compression(ddoc):
            # Just copy, don't read convert write
            # We could do shutil.move here, but... what if we fail while moving?
            # Also, a bit harder with doing the datadocs right
            self.log.info(f'Copy (skip recompression) for {dir_in}')
            if os.path.exists(dir_out):
                self.remove_dir(dir_out)
            shutil.copytree(dir_in, dir_out, dirs_exist_ok=True)
            return

        self.log.info(f'Start {dir_in} -> {dir_out}')
        compressor, target_size_mb = self.get_compressor_and_size(ddoc)

        summary = strax.rechunker(
            source_directory=dir_in,
            dest_directory=self.write_to,
            replace=False,
            compressor=compressor,
            target_size_mb=target_size_mb,
            rechunk=True,
        )
        self.log.info(f'{dir_out} written {summary}')

    def get_compressor_and_size(self, ddoc):
        # This is where we might do some fancy coding
        if ddoc['type'] in self.raw_record_types:
            compressor = 'lz4'
            target_size_mb = 5000
        else:
            compressor = 'zstd'
            target_size_mb = 1000
        return compressor, target_size_mb

    def should_skip_compression(self, ddoc) -> bool:
        """Should we skip recompressing this data? For example if the data is already so small that it's not worth recompressing"""
        if ddoc['type'] == 'live_data':
            return True
        return any(fnmatch.fnmatch(ddoc['type'], delete)
                   for delete in self.skip_compression)

    def do_checks(self, ddoc):
        if self.trump_mode:
            # One does not just venture into Mar a lago!
            return
        storage_backend = strax.FileSytemBackend()
        for doc in ddoc:
            dir_in = doc['location']
            dir_out = self.renamed_path(dir_in)
            errors = []

            if not os.path.exists(dir_in): errors.append(f'{dir_in} does not exists.')
            if not os.path.exists(dir_out): errors.append(f'{dir_out} does not exists.')
            md_in = storage_backend.get_metadata(dir_in)
            md_out = storage_backend.get_metadata(dir_out)

            if 'exception' in md_out:
                errors.append('Writing error!')

            if sum(chunk['n'] for chunk in md_in['chunks']) != sum(chunk['n'] for chunk in md_out['chunks']):
                errors.append('Rechunked data has fewer entries?!')
            if self.deep_compare:

                is_large_dset = sum(chunk['nbytes'] for chunk in md_in['chunks']) > 5e9

                self.log.info(f'Checking {dir_in} vs {dir_out}. Is large {is_large_dset}')
                kw = dict(keep_column='time' if is_large_dset else None, progress_bar=is_large_dset)
                data_in = self.get_data_from_dir(dir_in, **kw)
                data_out = self.get_data_from_dir(dir_out, **kw)
                if data_in.dtype.names:
                    for field in data_in.dtype.names:
                        if 'str' in data_in.dtype[field].name:
                            if not np.array_equal(data_in[field], data_out[field]):
                                errors.append(f'Field "{field}" different for {dir_in} vs {dir_out}')
                        else:
                            if not np.array_equal(data_in[field], data_out[field], equal_nan=True):
                                errors.append(f'Field "{field}" different for {dir_in} vs {dir_out}')
                elif not np.array_equal(data_in, data_out, equal_nan=True):
                    errors.append(f'{dir_in} vs {dir_out} different')
        if errors:
            raise ValueError(f'Doc {doc} had errors: ' + ' and '.join(errors))
        self.log.info('All checks passed')

    def get_data_from_dir(self, folder, keep_column=None, progress_bar=False):
        self.log.debug(f'Load {folder}, keep_column = "{keep_column if keep_column else "ALL"}"')
        loader = strax.utils.tqdm(strax.FileSytemBackend().loader(folder), disable=not progress_bar)
        if keep_column:
            return np.concatenate([chunk.data[keep_column] for chunk in loader])
        return np.concatenate([chunk.data for chunk in loader])

    def finalize_execute(self, ddocs):
        # Maybe could merge this with do checks? -> Avoid opening metadate twice?
        # Then again, that is SO minor in the grand scheme of things, that I just leave it like this for the moment
        if not self.production or not len(ddocs):
            return
        storage_backend = strax.FileSytemBackend()
        for ddoc in ddocs:
            if self.hostname != ddoc['host']:
                continue
            dir_in = ddoc.get('location', '')
            dir_out = self.renamed_path(dir_in)
            file_count = len(os.listdir(dir_out))
            md = storage_backend.get_metadata(dir_out)
            run_id = md['run_id']
            chunk_mb = [chunk['nbytes'] / 1e6 for chunk in md['chunks']]
            data_size_mb = int(np.sum(chunk_mb))
            avg_data_size_mb = int(np.average(chunk_mb))
            lineage_hash = md['lineage_hash']
            compressor = md['compressor']

            if self.production:
                self.log.info(f'Update ddoc {dir_out}')
                self.run_coll.update_one(
                    {'number': int(run_id),
                     'data': {
                         '$elemMatch': {
                             'location': ddoc['location'],
                             'host': ddoc['host']}}},
                    {'$set':
                     # TODO check that this new location is correct!
                         {'data.$.location': dir_out,
                          'data.$.file_count': file_count,
                          'data.$.meta.strax_version': strax.__version__,
                          'data.$.meta.straxen_version': straxen.__version__,
                          'data.$.meta.size_mb': data_size_mb,
                          'data.$.meta.avg_chunk_mb': avg_data_size_mb,
                          'data.$.meta.lineage_hash': lineage_hash,
                          'data.$.meta.compressor': compressor,
                          }
                     })
        # Mark as ready for upload
        if self.production:
            self.run_coll.update_one(
                {'number': int(run_id)},
                {'$set': {'status': 'eb_ready_to_upload'}})
        self.log.info('Rundoc updated')

    def take_a_nap(self, dt=None):
        time.sleep(dt if dt is not None else self.nap_time)

    def renamed_path(self, path):
        return os.path.join(self.write_to, os.path.split(path)[-1])

    def set_logger(self):
        versions = straxen.print_versions(
            modules='strax straxen utilix daqnt numpy tensorflow numba'.split(),
            include_git=True,
            return_string=True,
        )
        log_name = 'restrax' + self.hostname + ('' if self.production else '_TESTING')
        self.log = daqnt.get_daq_logger(
            log_name,
            log_name,
            level=logging.DEBUG,
            opening_message=f'Welcome to restrax\n{versions}')

    def remove_dir(self, directory):
        """Remove directory (when in production mode)"""
        self.log.info(f'Remove {directory}')
        if self.production:
            shutil.rmtree(directory)

    def log_warning(self, message, **kw):
        self.log.warning(message)
        for key, value in dict(production=self.production, user=f'restrax_{self.hostname}').items():
            kw.setdefault(key, value)
        super().log_warning(message, **kw)


if __name__ == '__main__':
    start = time.time()
    mem = memory_usage(proc=(main,))
    print(f"Memory profiler says peak RAM usage was: {max(mem):.1f} MB")
    print(f'Took {time.time() - start:.1f} s = {(time.time() - start) / 3600:.2f} h ')
    print('Bye, bye')
