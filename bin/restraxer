#!/usr/bin/env python
"""
Restrax: Rechunking live data
=============================================
How to use
----------------
    <activate conda environment>
    restrax --production

----------------

For more info, see the documentation:
https://straxen.readthedocs.io/en/latest/scripts.html
"""
__version__ = '0.0.0'

import argparse
import logging
import os
import immutabledict
import socket
import shutil
import time
import numpy as np
import strax
import straxen
import threading
import daqnt
import fnmatch
import daq_core


def parse_args():
    parser = argparse.ArgumentParser(description="XENONnT rechunking manager")
    parser.add_argument('--production', action='store_true',
                        help="Run restrax in production mode.")
    parser.add_argument('--ignore_checks', action='store_true',
                        help="Do not use! Skip checks before changing documents there be dragons.")
    parser.add_argument('--input_folder', type=str, default=daq_core.pre_folder,
                        help="Where to read the data from, should only be used when testing", )
    parser.add_argument('--max_threads', type=int, default=1,
                        help='max number of threads to simultaniously work on one run'
                        )
    actions = parser.add_mutually_exclusive_group()
    actions.add_argument(
        '--undying', action='store_true',
        help="Except any error and ignore it")
    args = parser.parse_args()
    if args.input_folder != '/data/pre_processed' and args.production:
        raise ValueError(
            "Thou shall not pass, don't upload files from non production folders, what are you thinking you're doing?!!")
    return args


def main():
    args = parse_args()
    while True:
        try:
            run_restrax(args)
        except (KeyboardInterrupt, SystemExit):
            raise
        except Exception as fatal_error:
            # Open a class, just to log the error
            re_temp = ReStrax(args=args)
            re_temp.log.error(f'Fatal warning:\tran into {fatal_error}. Try '
                              f'logging error and restart restrax')
            try:
                re_temp.log_warning(
                    f'Fatal warning:\tran into {fatal_error}',
                    priority='error',
                    user=f'restrax_{re_temp.hostname}')
            except Exception as warning_error:
                re_temp.error(f'Fatal warning:\tcould not log {warning_error}')

            if not args.undying:
                raise

            # This usually only takes a minute or two
            time.sleep(60)
            re_temp.warning('Restarting main loop')


def run_restrax(args):
    """Run the inifinite loop of restrax"""
    restrax = ReStrax(args=args)
    restrax.infinite_loop()


class ReStrax(daq_core.DataBases):
    """
    Restrex: rechunking the data from bootstrax and prepare it for redax
    """
    log: logging.Logger
    nap_time = 300  # s
    nap_time_short = 5  # s
    folders = immutabledict.immutabledict(
        production_out='/data/xenonnt_processed',
        test_out='/data/test_processed')
    # TODO probably this should be an argument?
    skip_compression = ('*event*', '*peak*')
    raw_record_types = ('raw_records', 'raw_records_nv', 'raw_records_mv', 'records', 'records_nvrecords_mv')

    def __int__(self, args):
        super().__init__(production=args.production)

        self.hostname = socket.getfqdn()
        if args.production:
            self.read_from = None  # get from the database
            self.write_to = self.folders['production_out']
        else:
            self.read_from = args.input_folder
            self.write_to = self.folders['test_out']
        self.max_threads = args.max_threads
        self.trump_mode = args.ignore_checks

    def infinite_loop(self):
        while True:
            ddocs = self.find_work()
            if not ddocs:
                self.take_a_nap()
                continue
            self.do_work(ddocs)
            self.do_checks(ddocs)
            self.finalize_execute(ddocs)

    def find_work(self):
        if self.production:
            rund_doc = self.run_coll.find_one(
                {'status': 'eb_finished_pre',
                 'bootstrax.state': 'done',
                 'bootstrax.host': self.hostname
                 },
                projection={'data': 1, 'number': 1}
            )
            ddocs = [doc for doc in rund_doc['data'] if
                     doc.get('host') == self.hostname
                     ]
            self.log.info(f'{rund_doc["number"]} -> doing {len(ddocs)}')
            return ddocs

        # Just try to do all the folders, we are just doing a test
        folders = os.listdir(self.read_from)
        first_run = None
        ddocs = []
        for folder in folders:
            if len(split := folder.split('-')) and len(split[0]) == 6:
                run_id, data_type, lineage = split
                if first_run is None:
                    first_run = run_id
                if run_id != first_run:
                    continue
                self.log.info(f'Do {folder}')
                # TODO check if these were indeed the correct keys!
                ddocs.append({'host': self.hostname,
                              'location': os.path.join(self.read_from, folder),
                              'data_type': data_type,
                              'linage_hash': lineage,
                              })
        self.log.info(f'Total of {len(ddocs)} data documents')
        return ddocs

    def do_work(self, ddocs):
        """ For each of the data documents, rechunk/recompress/move the data """
        if self.max_threads > 1:
            threads = []
            for next_doc in ddocs:
                self._sleep_while_n_threads_alive(threads, self.max_threads)
                thread = threading.Thread(
                    target=self._do_work_per_ddoc,
                    args=(self, next_doc))
                thread.start()
                threads.append(thread)
            # Wait until all threads have finished
            self._sleep_while_n_threads_alive(threads, 0)
        else:
            for ddoc in ddocs:
                self._do_work_per_ddoc(ddoc)

    def _sleep_while_n_threads_alive(self, threads, n):
        while sum(t.is_alive() for t in threads) >= n:
            self.take_a_nap(self.nap_time_short)

    def _do_work_per_ddoc(self, ddoc):
        dir_in = ddoc['location']
        dir_out = self.renamed_path(dir_in)
        if not os.path.exists(dir_in):
            raise FileNotFoundError(ddoc)

        if self.skip_compression(ddoc):
            # Just copy, don't read convert write
            # We could do shutil.move here, but... what if we fail while moving? Also, a bit harder with doing the datadocs right
            self.log.info(f'Copy (skip recompression) for {dir_in}')
            shutil.copy(dir_in, dir_out)
            return
        self.log.info(f'Start {dir_in} -> {dir_out}')
        compressor, target_size_mb = self.get_compressor_and_size(ddoc)

        summary = strax.rechunker(
            source=dir_in,
            dest_directory=self.write_to,
            replace=False,
            compressor=compressor,
            target_size_mb=target_size_mb,
            rechunk=True,
        )
        self.log.info(f'{dir_out} written {summary}')

    def get_compressor_and_size(self, ddoc):
        # This is where we might do some fancy coding
        if ddoc['data_type'] in self.raw_record_types:
            compressor = 'bz2'
            target_size_mb = 5000
        else:
            compressor = 'zstd'
            target_size_mb = 1000
        return compressor, target_size_mb

    def skip_compression(self, ddoc) -> bool:
        """Should we skip recompressing this data? For example if the data is already so small that it's not worth recompressing"""
        return any(fnmatch.fnmatch(ddoc['data_type'], delete) for delete in self.skip_compression)

    def do_checks(self, ddoc):
        if self.trump_mode:
            # One does not just venture into Mar a lago!
            return
        storage_backend = strax.FileSytemBackend()
        for doc in ddoc:
            dir_in = doc['location']
            dir_out = self.renamed_path(dir_in)
            errors = []

            if not os.path.exists(dir_in): errors.append(f'{dir_in} does not exists.')
            if not os.path.exists(dir_out): errors.append(f'{dir_out} does not exists.')
            md_in = storage_backend.get_metadat(dir_in)
            md_out = storage_backend.get_metadat(dir_out)
            if 'exception' in md_out: errors.append('Writing error!')

            # TODO was this the right format of the metadata
            if sum(chunk['n'] for chunk in md_in['data']) != sum(chunk['n'] for chunk in md_out['chunks']):
                errors.append('Rechunked data has fewer entries?!')
            if errors:
                raise ValueError(f'Doc {doc} had errors: ' + ' and '.join(errors))

    def finalize_execute(self, ddocs):
        # Maybe could merge this with do checks? -> Avoid opening metadate twice?
        # Then again, that is SO minor in the grand scheme of things, that I just leave it like this for the moment
        if not self.production or not len(ddocs):
            return
        storage_backend = strax.FileSytemBackend()
        for ddoc in ddocs:
            if self.hostname != ddoc['host']:
                continue
            dir_in = ddoc.get('location', '')
            dir_out = self.renamed_path(dir_in)
            file_count = len(os.listdir(dir_out))
            md = storage_backend.get_metadat(dir_out)
            run_id = md['run_id']
            chunk_mb = [chunk['nbytes'] / 1e6 for chunk in md['chunks']]
            data_size_mb = int(np.sum(chunk_mb))
            avg_data_size_mb = int(np.average(chunk_mb))
            lineage_hash = md['lineage_hash']
            compressor = md['compressor']

            self.run_coll.update_one(
                {'number': int(run_id),
                 'data': {
                     '$elemMatch': {
                         'location': ddoc['location'],
                         'host': ddoc['host']}}},
                {'$set':
                 # TODO check that this new location is correct!
                     {'data.$.location': dir_out,
                      'data.$.file_count': file_count,
                      'data.$.meta.strax_version': strax.__version__,
                      'data.$.meta.straxen_version': straxen.__version__,
                      'data.$.meta.size_mb': data_size_mb,
                      'data.$.meta.avg_chunk_mb': avg_data_size_mb,
                      'data.$.meta.lineage_hash': lineage_hash,
                      'data.$.meta.compressor': compressor,
                      }
                 })
        # Mark as ready for upload
        self.run_coll.update_one(
            {'number': int(run_id)},
            {'$set': {'status': 'eb_ready_to_upload'}})

    def take_a_nap(self, dt=None):
        time.sleep(dt if dt is not None else self.nap_time)

    def renamed_path(self, path):
        return os.path.join(self.write_to, os.path.split(path)[-1])

    def set_logger(self):
        log_name = 'restrax' + self.hostname + ('' if self.production else '_TESTING')
        self.log = daqnt.get_daq_logger(
            log_name,
            log_name,
            level=logging.DEBUG,
            opening_message=f'Welcome to restrax-the script that saved computing')


if __name__ == '__main__':
    main()
